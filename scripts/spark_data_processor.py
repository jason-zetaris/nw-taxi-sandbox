#!/usr/bin/env python3
"""
PySpark-based data processing for NYC TLC data
Enhanced version of the original data download script using Spark for big data processing
"""

import sys
import os
from datetime import datetime, timedelta
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import requests

# Add scripts directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from spark_setup import create_spark_session, configure_spark_for_taxi_data

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SparkTaxiDataProcessor:
    """
    PySpark-based processor for NYC TLC Trip Record Data
    """
    
    def __init__(self, app_name="NYC_Taxi_Data_Processor", memory="6g"):
        """
        Initialize Spark session and configuration
        
        Args:
            app_name: Name of the Spark application
            memory: Driver memory allocation
        """
        self.spark = create_spark_session(app_name, memory)
        configure_spark_for_taxi_data(self.spark)
        logger.info("Spark session initialized for taxi data processing")
    
    def download_file(self, url, local_path):
        """Download a file from URL to local path"""
        try:
            logger.info(f"Downloading {url}")
            response = requests.get(url, stream=True)
            response.raise_for_status()
            
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            logger.info(f"Downloaded to {local_path}")
            return True
        except Exception as e:
            logger.error(f"Failed to download {url}: {e}")
            return False
    
    def download_zone_lookup(self):
        """Download taxi zone lookup data"""
        zone_url = "https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv"
        local_path = "../data/taxi_zone_lookup.csv"
        return self.download_file(zone_url, local_path)
    
    def download_monthly_data(self, data_type, year, month):
        """Download monthly trip data for a specific type"""
        data_types = {
            'yellow': 'yellow_tripdata',
            'green': 'green_tripdata'
        }
        
        if data_type not in data_types:
            raise ValueError(f"Invalid data type. Choose from: {list(data_types.keys())}\")\n        \n        base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n        filename = f\"{data_types[data_type]}_{year}-{month:02d}.parquet\"\n        url = base_url + filename\n        local_path = f\"../data/raw/{filename}\"\n        \n        return self.download_file(url, local_path)\n    \n    def process_yellow_taxi_data(self, file_path):\n        \"\"\"\n        Process yellow taxi data using Spark with optimized transformations\n        \n        Args:\n            file_path: Path to the parquet file\n            \n        Returns:\n            Spark DataFrame\n        \"\"\"\n        try:\n            logger.info(f\"Processing yellow taxi data from {file_path}\")\n            \n            # Read parquet file with Spark\n            df = self.spark.read.parquet(file_path)\n            initial_count = df.count()\n            logger.info(f\"Loaded {initial_count:,} records from {file_path}\")\n            \n            # Define column mappings\n            column_mapping = {\n                'tpep_pickup_datetime': 'pickup_datetime',\n                'tpep_dropoff_datetime': 'dropoff_datetime',\n                'PULocationID': 'pickup_location_id',\n                'DOLocationID': 'dropoff_location_id',\n                'passenger_count': 'passenger_count',\n                'trip_distance': 'trip_distance',\n                'fare_amount': 'fare_amount',\n                'extra': 'extra',\n                'mta_tax': 'mta_tax',\n                'tip_amount': 'tip_amount',\n                'tolls_amount': 'tolls_amount',\n                'improvement_surcharge': 'improvement_surcharge',\n                'total_amount': 'total_amount',\n                'payment_type': 'payment_type',\n                'RatecodeID': 'rate_code_id',\n                'store_and_fwd_flag': 'store_and_fwd_flag',\n                'VendorID': 'vendor_id',\n                'congestion_surcharge': 'congestion_surcharge',\n                'airport_fee': 'airport_fee'\n            }\n            \n            # Rename columns that exist\n            for old_col, new_col in column_mapping.items():\n                if old_col in df.columns:\n                    df = df.withColumnRenamed(old_col, new_col)\n            \n            # Add missing columns with default values\n            required_columns = {\n                'pickup_datetime': None,\n                'dropoff_datetime': None,\n                'pickup_location_id': None,\n                'dropoff_location_id': None,\n                'passenger_count': lit(1),\n                'trip_distance': lit(0.0),\n                'fare_amount': lit(0.0),\n                'extra': lit(0.0),\n                'mta_tax': lit(0.0),\n                'tip_amount': lit(0.0),\n                'tolls_amount': lit(0.0),\n                'improvement_surcharge': lit(0.0),\n                'total_amount': lit(0.0),\n                'payment_type': lit(1),\n                'rate_code_id': lit(1),\n                'store_and_fwd_flag': lit('N'),\n                'vendor_id': lit(1),\n                'congestion_surcharge': lit(0.0),\n                'airport_fee': lit(0.0)\n            }\n            \n            for col_name, default_value in required_columns.items():\n                if col_name not in df.columns and default_value is not None:\n                    df = df.withColumn(col_name, default_value)\n            \n            # Add trip type identifier\n            df = df.withColumn('trip_type', lit('yellow'))\n            \n            # Data quality filters using Spark SQL\n            df_filtered = df.filter(\n                (col('pickup_datetime') < col('dropoff_datetime')) &\n                (col('trip_distance') > 0) &\n                (col('trip_distance') < 100) &\n                (col('fare_amount') > 0) &\n                (col('total_amount') > 0) &\n                (col('passenger_count') > 0) &\n                (col('passenger_count') <= 6) &\n                (col('pickup_location_id').isNotNull()) &\n                (col('dropoff_location_id').isNotNull())\n            )\n            \n            # Fill null values\n            df_filtered = df_filtered.fillna({\n                'passenger_count': 1,\n                'extra': 0.0,\n                'mta_tax': 0.0,\n                'tip_amount': 0.0,\n                'tolls_amount': 0.0,\n                'improvement_surcharge': 0.0,\n                'congestion_surcharge': 0.0,\n                'airport_fee': 0.0,\n                'payment_type': 1,\n                'rate_code_id': 1,\n                'store_and_fwd_flag': 'N',\n                'vendor_id': 1\n            })\n            \n            filtered_count = df_filtered.count()\n            retention_rate = (filtered_count / initial_count) * 100\n            logger.info(f\"Filtered data: {initial_count:,} -> {filtered_count:,} records ({retention_rate:.1f}% retained)\")\n            \n            return df_filtered\n            \n        except Exception as e:\n            logger.error(f\"Error processing yellow taxi data from {file_path}: {e}\")\n            return None\n    \n    def process_green_taxi_data(self, file_path):\n        \"\"\"\n        Process green taxi data using Spark with optimized transformations\n        \n        Args:\n            file_path: Path to the parquet file\n            \n        Returns:\n            Spark DataFrame\n        \"\"\"\n        try:\n            logger.info(f\"Processing green taxi data from {file_path}\")\n            \n            # Read parquet file with Spark\n            df = self.spark.read.parquet(file_path)\n            initial_count = df.count()\n            logger.info(f\"Loaded {initial_count:,} records from {file_path}\")\n            \n            # Green taxi column mapping\n            column_mapping = {\n                'lpep_pickup_datetime': 'pickup_datetime',\n                'lpep_dropoff_datetime': 'dropoff_datetime',\n                'PULocationID': 'pickup_location_id',\n                'DOLocationID': 'dropoff_location_id',\n                'passenger_count': 'passenger_count',\n                'trip_distance': 'trip_distance',\n                'fare_amount': 'fare_amount',\n                'extra': 'extra',\n                'mta_tax': 'mta_tax',\n                'tip_amount': 'tip_amount',\n                'tolls_amount': 'tolls_amount',\n                'improvement_surcharge': 'improvement_surcharge',\n                'total_amount': 'total_amount',\n                'payment_type': 'payment_type',\n                'RatecodeID': 'rate_code_id',\n                'store_and_fwd_flag': 'store_and_fwd_flag',\n                'VendorID': 'vendor_id',\n                'congestion_surcharge': 'congestion_surcharge',\n                'ehail_fee': 'ehail_fee'\n            }\n            \n            # Rename columns that exist\n            for old_col, new_col in column_mapping.items():\n                if old_col in df.columns:\n                    df = df.withColumnRenamed(old_col, new_col)\n            \n            # Add missing columns\n            if 'ehail_fee' not in df.columns:\n                df = df.withColumn('ehail_fee', lit(0.0))\n            \n            # Add trip type identifier\n            df = df.withColumn('trip_type', lit('green'))\n            \n            # Apply same filters as yellow taxi\n            df_filtered = df.filter(\n                (col('pickup_datetime') < col('dropoff_datetime')) &\n                (col('trip_distance') > 0) &\n                (col('trip_distance') < 100) &\n                (col('fare_amount') > 0) &\n                (col('total_amount') > 0) &\n                (col('passenger_count') > 0) &\n                (col('passenger_count') <= 6) &\n                (col('pickup_location_id').isNotNull()) &\n                (col('dropoff_location_id').isNotNull())\n            )\n            \n            # Fill null values\n            df_filtered = df_filtered.fillna({\n                'passenger_count': 1,\n                'extra': 0.0,\n                'mta_tax': 0.0,\n                'tip_amount': 0.0,\n                'tolls_amount': 0.0,\n                'improvement_surcharge': 0.0,\n                'ehail_fee': 0.0,\n                'payment_type': 1,\n                'rate_code_id': 1,\n                'store_and_fwd_flag': 'N',\n                'vendor_id': 1\n            })\n            \n            filtered_count = df_filtered.count()\n            retention_rate = (filtered_count / initial_count) * 100\n            logger.info(f\"Filtered data: {initial_count:,} -> {filtered_count:,} records ({retention_rate:.1f}% retained)\")\n            \n            return df_filtered\n            \n        except Exception as e:\n            logger.error(f\"Error processing green taxi data from {file_path}: {e}\")\n            return None\n    \n    def combine_and_save_data(self, data_frames, output_path):\n        \"\"\"\n        Combine multiple Spark DataFrames and save using optimized Spark operations\n        \n        Args:\n            data_frames: List of Spark DataFrames\n            output_path: Output path for combined data\n            \n        Returns:\n            Combined Spark DataFrame\n        \"\"\"\n        if not data_frames:\n            logger.warning(\"No data frames to combine\")\n            return None\n        \n        try:\n            logger.info(f\"Combining {len(data_frames)} DataFrames...\")\n            \n            # Get all column names across dataframes\n            all_columns = set()\n            for df in data_frames:\n                all_columns.update(df.columns)\n            \n            # Add missing columns to each DataFrame\n            standardized_dfs = []\n            for df in data_frames:\n                for col_name in all_columns:\n                    if col_name not in df.columns:\n                        df = df.withColumn(col_name, lit(None))\n                \n                # Reorder columns consistently\n                df = df.select(*sorted(all_columns))\n                standardized_dfs.append(df)\n            \n            # Union all DataFrames\n            combined_df = standardized_dfs[0]\n            for df in standardized_dfs[1:]:\n                combined_df = combined_df.union(df)\n            \n            # Sort by pickup time for better partitioning\n            combined_df = combined_df.orderBy('pickup_datetime')\n            \n            # Cache for performance\n            combined_df.cache()\n            \n            total_count = combined_df.count()\n            logger.info(f\"Combined dataset: {total_count:,} records\")\n            \n            # Save as partitioned parquet\n            os.makedirs(os.path.dirname(output_path), exist_ok=True)\n            \n            logger.info(f\"Saving combined data to {output_path}...\")\n            combined_df.write.mode(\"overwrite\").parquet(output_path)\n            \n            # Also save a sample as CSV for quick inspection\n            sample_path = output_path.replace('.parquet', '_sample.csv')\n            sample_df = combined_df.limit(10000)\n            sample_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(sample_path)\n            \n            logger.info(f\"Saved combined data to {output_path}\")\n            logger.info(f\"Saved sample data to {sample_path}\")\n            \n            return combined_df\n            \n        except Exception as e:\n            logger.error(f\"Error combining and saving data: {e}\")\n            return None\n    \n    def process_multiple_months(self, start_year, start_month, num_months=3):\n        \"\"\"\n        Process multiple months of data efficiently with Spark\n        \n        Args:\n            start_year: Starting year\n            start_month: Starting month\n            num_months: Number of months to process\n            \n        Returns:\n            Combined DataFrame\n        \"\"\"\n        logger.info(f\"Processing {num_months} months of data starting from {start_year}-{start_month:02d}\")\n        \n        data_frames = []\n        current_year = start_year\n        current_month = start_month\n        \n        for i in range(num_months):\n            logger.info(f\"Processing month {i+1}/{num_months}: {current_year}-{current_month:02d}\")\n            \n            # Download and process yellow taxi data\n            if self.download_monthly_data('yellow', current_year, current_month):\n                file_path = f\"../data/raw/yellow_tripdata_{current_year}-{current_month:02d}.parquet\"\n                yellow_df = self.process_yellow_taxi_data(file_path)\n                if yellow_df is not None:\n                    data_frames.append(yellow_df)\n            \n            # Download and process green taxi data\n            if self.download_monthly_data('green', current_year, current_month):\n                file_path = f\"../data/raw/green_tripdata_{current_year}-{current_month:02d}.parquet\"\n                green_df = self.process_green_taxi_data(file_path)\n                if green_df is not None:\n                    data_frames.append(green_df)\n            \n            # Move to next month\n            current_month += 1\n            if current_month > 12:\n                current_month = 1\n                current_year += 1\n        \n        # Combine all data\n        if data_frames:\n            output_path = \"../data/processed/nyc_taxi_trips_spark.parquet\"\n            combined_df = self.combine_and_save_data(data_frames, output_path)\n            return combined_df\n        else:\n            logger.error(\"No data was successfully processed\")\n            return None\n    \n    def generate_summary_statistics(self, df):\n        \"\"\"\n        Generate summary statistics using Spark SQL\n        \n        Args:\n            df: Spark DataFrame\n        \"\"\"\n        if df is None:\n            return\n        \n        logger.info(\"Generating summary statistics...\")\n        \n        # Create temporary view for SQL queries\n        df.createOrReplaceTempView(\"trips\")\n        \n        # Basic statistics\n        basic_stats = self.spark.sql(\"\"\"\n            SELECT \n                COUNT(*) as total_trips,\n                MIN(pickup_datetime) as earliest_trip,\n                MAX(pickup_datetime) as latest_trip,\n                COUNT(DISTINCT trip_type) as trip_types,\n                AVG(trip_distance) as avg_distance,\n                AVG(fare_amount) as avg_fare,\n                AVG(total_amount) as avg_total,\n                COUNT(DISTINCT pickup_location_id) as unique_pickup_zones,\n                COUNT(DISTINCT dropoff_location_id) as unique_dropoff_zones\n            FROM trips\n        \"\"\").collect()[0]\n        \n        # Trip type distribution\n        trip_type_dist = self.spark.sql(\"\"\"\n            SELECT trip_type, COUNT(*) as count\n            FROM trips\n            GROUP BY trip_type\n            ORDER BY count DESC\n        \"\"\").collect()\n        \n        # Print summary\n        print(\"\\n\" + \"=\"*60)\n        print(\"NYC TAXI DATA SUMMARY (Processed with Spark)\")\n        print(\"=\"*60)\n        print(f\"Total trips: {basic_stats['total_trips']:,}\")\n        print(f\"Date range: {basic_stats['earliest_trip']} to {basic_stats['latest_trip']}\")\n        print(f\"Average trip distance: {basic_stats['avg_distance']:.2f} miles\")\n        print(f\"Average fare: ${basic_stats['avg_fare']:.2f}\")\n        print(f\"Average total amount: ${basic_stats['avg_total']:.2f}\")\n        print(f\"Unique pickup locations: {basic_stats['unique_pickup_zones']}\")\n        print(f\"Unique dropoff locations: {basic_stats['unique_dropoff_zones']}\")\n        \n        print(f\"\\nTrip type distribution:\")\n        for row in trip_type_dist:\n            print(f\"  {row['trip_type']}: {row['count']:,}\")\n        \n        print(\"\\nâœ… Data processing completed successfully with Spark!\")\n    \n    def stop(self):\n        \"\"\"Stop the Spark session\"\"\"\n        if self.spark:\n            self.spark.stop()\n            logger.info(\"Spark session stopped\")\n\ndef main():\n    \"\"\"Main function to run the Spark-based data processing\"\"\"\n    processor = None\n    \n    try:\n        # Initialize processor\n        processor = SparkTaxiDataProcessor(\"NYC_Taxi_Data_Processing\", \"8g\")\n        \n        # Download zone lookup data\n        logger.info(\"Downloading zone lookup data...\")\n        processor.download_zone_lookup()\n        \n        # Calculate date range (last 3 months)\n        end_date = datetime.now().replace(day=1)\n        start_date = end_date - timedelta(days=90)\n        \n        # Process data\n        combined_df = processor.process_multiple_months(\n            start_date.year, \n            start_date.month, \n            num_months=3\n        )\n        \n        # Generate summary statistics\n        if combined_df:\n            processor.generate_summary_statistics(combined_df)\n        \n    except Exception as e:\n        logger.error(f\"Error in main processing: {e}\")\n    \n    finally:\n        # Clean up\n        if processor:\n            processor.stop()\n\nif __name__ == \"__main__\":\n    main()