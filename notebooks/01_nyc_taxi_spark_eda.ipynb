{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Data Analysis with PySpark\n",
    "\n",
    "This notebook provides comprehensive big data analysis of NYC TLC Trip Record Data using **Apache Spark**.\n",
    "\n",
    "**Data Source**: [NYC Taxi and Limousine Commission](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "\n",
    "**Technologies Used**:\n",
    "- **PySpark**: Distributed data processing\n",
    "- **Spark SQL**: Large-scale SQL analytics\n",
    "- **MLlib**: Machine learning at scale\n",
    "\n",
    "**Analysis Includes**:\n",
    "- Scalable data processing and ETL\n",
    "- Distributed analytics and aggregations\n",
    "- Advanced SQL queries on big data\n",
    "- Performance optimization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import our Spark setup utilities\n",
    "from spark_setup import (\n",
    "    create_spark_session,\n",
    "    configure_spark_for_taxi_data,\n",
    "    print_spark_ui_info,\n",
    "    cache_dataframe,\n",
    "    create_temp_views\n",
    ")\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "print(\"üöÄ NYC Taxi Big Data Analysis with PySpark\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "print(\"üîß Initializing Spark Session...\")\n",
    "\n",
    "# Create Spark session with optimized configuration\n",
    "spark = create_spark_session(\n",
    "    app_name=\"NYC_Taxi_Analytics\",\n",
    "    memory=\"6g\",  # Adjust based on your system\n",
    "    cores=\"*\"\n",
    ")\n",
    "\n",
    "# Configure for taxi data analysis\n",
    "configure_spark_for_taxi_data(spark)\n",
    "\n",
    "# Print Spark UI information\n",
    "print_spark_ui_info(spark)\n",
    "\n",
    "print(\"\\n‚úÖ Spark initialization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Access NYC TLC data from Spark tables with nw_taxi namespace\nprint(\"üìÇ Accessing NYC TLC Trip Data from Spark Tables...\")\n\n# Check available tables in Spark catalog\ntry:\n    available_tables = spark.sql(\"SHOW TABLES\").collect()\n    print(\"Available Spark tables:\")\n    for table in available_tables:\n        print(f\"  - {table.tableName}\")\nexcept Exception as e:\n    print(f\"Note: {e}\")\n    print(\"Checking for default database tables...\")\n\n# Try to access trip data from Spark table with namespace\ntry:\n    print(\"\\nAccessing trip data from Spark table...\")\n    trips_df = spark.sql(\"SELECT * FROM nw_taxi.trips\")\n    trip_count = spark.sql(\"SELECT COUNT(*) as count FROM nw_taxi.trips\").collect()[0]['count']\n    print(f\"‚úÖ Accessed trip data: {trip_count:,} trips\")\nexcept Exception as e:\n    print(f\"‚ùå Could not access 'nw_taxi.trips' table: {e}\")\n    print(\"Creating temporary table from sample data...\")\n    # Fallback: create sample data if table doesn't exist\n    from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n    sample_schema = StructType([\n        StructField(\"trip_id\", IntegerType(), True),\n        StructField(\"trip_type\", StringType(), True),\n        StructField(\"pickup_datetime\", TimestampType(), True),\n        StructField(\"dropoff_datetime\", TimestampType(), True),\n        StructField(\"pickup_location_id\", IntegerType(), True),\n        StructField(\"dropoff_location_id\", IntegerType(), True),\n        StructField(\"fare_amount\", DoubleType(), True),\n        StructField(\"total_amount\", DoubleType(), True),\n        StructField(\"tip_amount\", DoubleType(), True),\n        StructField(\"trip_distance\", DoubleType(), True),\n        StructField(\"payment_type\", IntegerType(), True)\n    ])\n    trips_df = spark.createDataFrame([], sample_schema)\n    trips_df.createOrReplaceTempView(\"trips\")\n    trip_count = 0\n\n# Try to access zone lookup data from Spark table with namespace\ntry:\n    print(\"Accessing zone lookup data from Spark table...\")\n    zones_df = spark.sql(\"SELECT * FROM nw_taxi.zones\")\n    zone_count = spark.sql(\"SELECT COUNT(*) as count FROM nw_taxi.zones\").collect()[0]['count']\n    print(f\"‚úÖ Accessed zone data: {zone_count:,} zones\")\nexcept Exception as e:\n    print(f\"‚ùå Could not access 'nw_taxi.zones' table: {e}\")\n    print(\"Creating temporary zones table...\")\n    # Fallback: create sample zones if table doesn't exist\n    from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n    zones_schema = StructType([\n        StructField(\"location_id\", IntegerType(), True),\n        StructField(\"Borough\", StringType(), True),\n        StructField(\"Zone\", StringType(), True),\n        StructField(\"service_zone\", StringType(), True)\n    ])\n    zones_df = spark.createDataFrame([], zones_schema)\n    zones_df.createOrReplaceTempView(\"zones\")\n    zone_count = 0\n\n# Alternative: Check if data exists in different table names with namespace\nif trip_count == 0:\n    print(\"\\nüîç Searching for alternative table names...\")\n    alternative_tables = ['nw_taxi.taxi_trips', 'nw_taxi.nyc_trips', 'nw_taxi.trip_data', 'nw_taxi.yellow_trips', 'nw_taxi.green_trips']\n    \n    for table_name in alternative_tables:\n        try:\n            test_df = spark.sql(f\"SELECT COUNT(*) as count FROM {table_name}\")\n            count = test_df.collect()[0]['count']\n            if count > 0:\n                print(f\"‚úÖ Found data in table '{table_name}': {count:,} records\")\n                trips_df = spark.sql(f\"SELECT * FROM {table_name}\")\n                trips_df.createOrReplaceTempView(\"trips\")\n                trip_count = count\n                break\n        except:\n            continue\n\nif trip_count > 0:\n    # Cache the DataFrames for better performance\n    trips_df = cache_dataframe(trips_df, \"MEMORY_AND_DISK\")\n    zones_df = cache_dataframe(zones_df, \"MEMORY_ONLY\")\n    print(\"üíæ Data cached for optimized performance\")\nelse:\n    print(\"‚ö†Ô∏è No trip data found in Spark tables. Please ensure data is loaded into Spark first.\")\n    print(\"   You can load data using: python scripts/spark_data_processor.py\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data schema and structure\n",
    "print(\"üìã Trip Data Schema:\")\n",
    "trips_df.printSchema()\n",
    "\n",
    "print(\"\\nüìã Zone Data Schema:\")\n",
    "zones_df.printSchema()\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nüìä Sample Trip Data:\")\n",
    "trips_df.show(5, truncate=False)\n",
    "\n",
    "print(\"\\nüìç Sample Zone Data:\")\n",
    "zones_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment using Spark\n",
    "print(\"üîç Data Quality Assessment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Basic statistics\n",
    "total_trips = trips_df.count()\n",
    "date_range = trips_df.select(\n",
    "    min(\"pickup_datetime\").alias(\"min_date\"),\n",
    "    max(\"pickup_datetime\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"üìä Dataset Overview:\")\n",
    "print(f\"   Total trips: {total_trips:,}\")\n",
    "print(f\"   Date range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "\n",
    "# Trip type distribution\n",
    "trip_type_dist = trips_df.groupBy(\"trip_type\").count().orderBy(desc(\"count\"))\n",
    "print(f\"\\nüöñ Trip Type Distribution:\")\n",
    "trip_type_dist.show()\n",
    "\n",
    "# Data quality checks\n",
    "print(\"\\nüîç Data Quality Checks:\")\n",
    "\n",
    "# Check for nulls in critical columns\n",
    "critical_columns = [\"pickup_datetime\", \"dropoff_datetime\", \"pickup_location_id\", \n",
    "                   \"dropoff_location_id\", \"trip_distance\", \"fare_amount\", \"total_amount\"]\n",
    "\n",
    "for col in critical_columns:\n",
    "    null_count = trips_df.filter(trips_df[col].isNull()).count()\n",
    "    null_pct = (null_count / total_trips) * 100\n",
    "    print(f\"   {col}: {null_count:,} nulls ({null_pct:.2f}%)\")\n",
    "\n",
    "# Basic trip statistics\n",
    "trip_stats = trips_df.select(\n",
    "    avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "    avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "    avg(\"total_amount\").alias(\"avg_total\"),\n",
    "    avg(\"tip_amount\").alias(\"avg_tip\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nüìà Trip Statistics:\")\n",
    "print(f\"   Average distance: {trip_stats['avg_distance']:.2f} miles\")\n",
    "print(f\"   Average fare: ${trip_stats['avg_fare']:.2f}\")\n",
    "print(f\"   Average total: ${trip_stats['avg_total']:.2f}\")\n",
    "print(f\"   Average tip: ${trip_stats['avg_tip']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Engineering - Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features using Spark SQL functions\n",
    "print(\"üîß Creating derived features...\")\n",
    "\n",
    "# Add time-based features\n",
    "trips_enriched = trips_df.withColumn(\"pickup_hour\", hour(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"pickup_day_of_week\", dayofweek(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"pickup_month\", month(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"pickup_year\", year(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"pickup_date\", to_date(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"is_weekend\", when(dayofweek(\"pickup_datetime\").isin([1, 7]), True).otherwise(False))\n",
    "\n",
    "# Add trip duration in minutes\n",
    "trips_enriched = trips_enriched.withColumn(\n",
    "    \"trip_duration_minutes\",\n",
    "    (unix_timestamp(\"dropoff_datetime\") - unix_timestamp(\"pickup_datetime\")) / 60\n",
    ")\n",
    "\n",
    "# Add trip speed (mph)\n",
    "trips_enriched = trips_enriched.withColumn(\n",
    "    \"trip_speed_mph\",\n",
    "    when(col(\"trip_duration_minutes\") > 0, \n",
    "         col(\"trip_distance\") / (col(\"trip_duration_minutes\") / 60))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Add tip percentage\n",
    "trips_enriched = trips_enriched.withColumn(\n",
    "    \"tip_percentage\",\n",
    "    when(col(\"fare_amount\") > 0, \n",
    "         (col(\"tip_amount\") / col(\"fare_amount\")) * 100)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Add fare per mile\n",
    "trips_enriched = trips_enriched.withColumn(\n",
    "    \"fare_per_mile\",\n",
    "    when(col(\"trip_distance\") > 0, \n",
    "         col(\"fare_amount\") / col(\"trip_distance\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Add time period classification\n",
    "trips_enriched = trips_enriched.withColumn(\n",
    "    \"time_period\",\n",
    "    when((col(\"pickup_hour\") >= 7) & (col(\"pickup_hour\") <= 9), \"Morning Rush\")\n",
    "    .when((col(\"pickup_hour\") >= 17) & (col(\"pickup_hour\") <= 19), \"Evening Rush\")\n",
    "    .when((col(\"pickup_hour\") >= 10) & (col(\"pickup_hour\") <= 16), \"Daytime\")\n",
    "    .when((col(\"pickup_hour\") >= 20) & (col(\"pickup_hour\") <= 23), \"Evening\")\n",
    "    .otherwise(\"Late Night/Early Morning\")\n",
    ")\n",
    "\n",
    "# Add day name\n",
    "trips_enriched = trips_enriched.withColumn(\n",
    "    \"day_name\",\n",
    "    when(col(\"pickup_day_of_week\") == 1, \"Sunday\")\n",
    "    .when(col(\"pickup_day_of_week\") == 2, \"Monday\")\n",
    "    .when(col(\"pickup_day_of_week\") == 3, \"Tuesday\")\n",
    "    .when(col(\"pickup_day_of_week\") == 4, \"Wednesday\")\n",
    "    .when(col(\"pickup_day_of_week\") == 5, \"Thursday\")\n",
    "    .when(col(\"pickup_day_of_week\") == 6, \"Friday\")\n",
    "    .when(col(\"pickup_day_of_week\") == 7, \"Saturday\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "\n",
    "# Add payment method description\n",
    "trips_enriched = trips_enriched.withColumn(\n",
    "    \"payment_method\",\n",
    "    when(col(\"payment_type\") == 1, \"Credit Card\")\n",
    "    .when(col(\"payment_type\") == 2, \"Cash\")\n",
    "    .when(col(\"payment_type\") == 3, \"No Charge\")\n",
    "    .when(col(\"payment_type\") == 4, \"Dispute\")\n",
    "    .when(col(\"payment_type\") == 5, \"Unknown\")\n",
    "    .when(col(\"payment_type\") == 6, \"Voided Trip\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "# Filter for valid trips\n",
    "valid_trips = trips_enriched.filter(\n",
    "    (col(\"pickup_datetime\") < col(\"dropoff_datetime\")) &\n",
    "    (col(\"trip_distance\") > 0) &\n",
    "    (col(\"trip_distance\") < 100) &\n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"total_amount\") > 0) &\n",
    "    (col(\"pickup_location_id\").isNotNull()) &\n",
    "    (col(\"dropoff_location_id\").isNotNull()) &\n",
    "    (col(\"trip_speed_mph\") >= 1) &\n",
    "    (col(\"trip_speed_mph\") <= 80)\n",
    ")\n",
    "\n",
    "# Cache the enriched dataset\n",
    "valid_trips = cache_dataframe(valid_trips, \"MEMORY_AND_DISK\")\n",
    "\n",
    "print(f\"‚úÖ Feature engineering complete\")\n",
    "print(f\"üìä Valid trips after filtering: {valid_trips.count():,}\")\n",
    "print(f\"üîß Added {len(trips_enriched.columns) - len(trips_df.columns)} new features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create temporary views for SQL analysis with nw_taxi namespace\n# Create database if it doesn't exist\nspark.sql(\"CREATE DATABASE IF NOT EXISTS nw_taxi\")\n\n# Register temporary views with namespace\nvalid_trips.createGlobalTempView(\"nw_taxi.trips\")\nzones_df.createGlobalTempView(\"nw_taxi.zones\")\nprint(\"üìä Created global temp views: nw_taxi.trips, nw_taxi.zones\")\n\n# Also create a joined view with zone information\ntrips_with_zones = valid_trips.alias(\"t\") \\\n    .join(zones_df.alias(\"pz\"), col(\"t.pickup_location_id\") == col(\"pz.location_id\"), \"left\") \\\n    .select(\n        col(\"t.*\"),\n        col(\"pz.Borough\").alias(\"pickup_borough\"),\n        col(\"pz.Zone\").alias(\"pickup_zone\")\n    ).join(zones_df.alias(\"dz\"), col(\"t.dropoff_location_id\") == col(\"dz.location_id\"), \"left\") \\\n    .select(\n        col(\"t.*\"),\n        col(\"pickup_borough\"),\n        col(\"pickup_zone\"),\n        col(\"dz.Borough\").alias(\"dropoff_borough\"),\n        col(\"dz.Zone\").alias(\"dropoff_zone\")\n    )\n\ntrips_with_zones.createGlobalTempView(\"nw_taxi.trips_with_zones\")\nprint(\"üìä Created global temp view: nw_taxi.trips_with_zones\")\n\n# Show sample of enriched data\nprint(\"\\nüìä Sample enriched data:\")\nvalid_trips.select(\n    \"pickup_datetime\", \"trip_type\", \"pickup_hour\", \"day_name\", \n    \"time_period\", \"trip_distance\", \"trip_duration_minutes\", \n    \"trip_speed_mph\", \"fare_amount\", \"tip_percentage\", \"payment_method\"\n).show(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Analysis with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Temporal analysis using Spark SQL\nprint(\"‚è∞ Temporal Demand Analysis\")\nprint(\"=\" * 40)\n\n# Hourly demand patterns\nhourly_demand = spark.sql(\"\"\"\n    SELECT \n        pickup_hour,\n        trip_type,\n        COUNT(*) as trip_count,\n        AVG(fare_amount) as avg_fare,\n        AVG(trip_distance) as avg_distance,\n        AVG(tip_percentage) as avg_tip_pct,\n        AVG(trip_speed_mph) as avg_speed\n    FROM nw_taxi.trips\n    GROUP BY pickup_hour, trip_type\n    ORDER BY pickup_hour, trip_type\n\"\"\")\n\nprint(\"üìä Hourly Demand Pattern:\")\nhourly_demand.show(24)\n\n# Daily patterns\ndaily_patterns = spark.sql(\"\"\"\n    SELECT \n        day_name,\n        trip_type,\n        COUNT(*) as trip_count,\n        AVG(fare_amount) as avg_fare,\n        AVG(trip_distance) as avg_distance,\n        SUM(total_amount) as total_revenue\n    FROM nw_taxi.trips\n    GROUP BY day_name, trip_type\n    ORDER BY \n        CASE day_name\n            WHEN 'Monday' THEN 1\n            WHEN 'Tuesday' THEN 2\n            WHEN 'Wednesday' THEN 3\n            WHEN 'Thursday' THEN 4\n            WHEN 'Friday' THEN 5\n            WHEN 'Saturday' THEN 6\n            WHEN 'Sunday' THEN 7\n        END,\n        trip_type\n\"\"\")\n\nprint(\"\\nüìÖ Daily Demand Pattern:\")\ndaily_patterns.show()\n\n# Peak hours identification\npeak_hours = spark.sql(\"\"\"\n    SELECT \n        pickup_hour,\n        SUM(CASE WHEN trip_type = 'yellow' THEN 1 ELSE 0 END) as yellow_trips,\n        SUM(CASE WHEN trip_type = 'green' THEN 1 ELSE 0 END) as green_trips,\n        COUNT(*) as total_trips,\n        AVG(fare_amount) as avg_fare\n    FROM nw_taxi.trips\n    GROUP BY pickup_hour\n    ORDER BY total_trips DESC\n    LIMIT 10\n\"\"\")\n\nprint(\"\\nüïê Top 10 Peak Hours:\")\npeak_hours.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrames to Pandas for visualization\n",
    "print(\"üìä Creating temporal visualizations...\")\n",
    "\n",
    "# Convert hourly data to Pandas for plotting\n",
    "hourly_pandas = hourly_demand.toPandas()\n",
    "daily_pandas = daily_patterns.toPandas()\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Hourly trip volume by trip type\n",
    "for trip_type in hourly_pandas['trip_type'].unique():\n",
    "    data = hourly_pandas[hourly_pandas['trip_type'] == trip_type]\n",
    "    axes[0, 0].plot(data['pickup_hour'], data['trip_count'], \n",
    "                   marker='o', linewidth=2, label=f'{trip_type.title()} Taxi')\n",
    "\n",
    "axes[0, 0].set_xlabel('Hour of Day')\n",
    "axes[0, 0].set_ylabel('Number of Trips')\n",
    "axes[0, 0].set_title('Hourly Trip Volume by Taxi Type')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Daily trip volume\n",
    "daily_pivot = daily_pandas.pivot(index='day_name', columns='trip_type', values='trip_count')\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daily_pivot.reindex(day_order).plot(kind='bar', ax=axes[0, 1], color=['gold', 'green'])\n",
    "axes[0, 1].set_xlabel('Day of Week')\n",
    "axes[0, 1].set_ylabel('Number of Trips')\n",
    "axes[0, 1].set_title('Daily Trip Volume by Taxi Type')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].legend(title='Taxi Type')\n",
    "\n",
    "# Hourly average speed\n",
    "for trip_type in hourly_pandas['trip_type'].unique():\n",
    "    data = hourly_pandas[hourly_pandas['trip_type'] == trip_type]\n",
    "    axes[1, 0].plot(data['pickup_hour'], data['avg_speed'], \n",
    "                   marker='s', linewidth=2, label=f'{trip_type.title()} Taxi')\n",
    "\n",
    "axes[1, 0].set_xlabel('Hour of Day')\n",
    "axes[1, 0].set_ylabel('Average Speed (mph)')\n",
    "axes[1, 0].set_title('Average Trip Speed by Hour')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Daily revenue\n",
    "daily_revenue = daily_pandas.pivot(index='day_name', columns='trip_type', values='total_revenue')\n",
    "daily_revenue.reindex(day_order).plot(kind='bar', ax=axes[1, 1], color=['gold', 'green'])\n",
    "axes[1, 1].set_xlabel('Day of Week')\n",
    "axes[1, 1].set_ylabel('Total Revenue ($)')\n",
    "axes[1, 1].set_title('Daily Revenue by Taxi Type')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].legend(title='Taxi Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Temporal analysis visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geographic Analysis with Distributed Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Geographic analysis using Spark SQL\nprint(\"üó∫Ô∏è Geographic Analysis\")\nprint(\"=\" * 40)\n\n# Borough-level analysis\nborough_stats = spark.sql(\"\"\"\n    SELECT \n        pickup_borough,\n        trip_type,\n        COUNT(*) as trip_count,\n        AVG(fare_amount) as avg_fare,\n        AVG(trip_distance) as avg_distance,\n        AVG(tip_percentage) as avg_tip_pct,\n        SUM(total_amount) as total_revenue,\n        AVG(trip_speed_mph) as avg_speed\n    FROM nw_taxi.trips_with_zones\n    WHERE pickup_borough IS NOT NULL\n    GROUP BY pickup_borough, trip_type\n    ORDER BY trip_count DESC\n\"\"\")\n\nprint(\"üèôÔ∏è Borough Statistics:\")\nborough_stats.show(truncate=False)\n\n# Top pickup zones\ntop_pickup_zones = spark.sql(\"\"\"\n    SELECT \n        pickup_zone,\n        pickup_borough,\n        COUNT(*) as trip_count,\n        AVG(fare_amount) as avg_fare,\n        SUM(total_amount) as total_revenue,\n        AVG(trip_distance) as avg_distance\n    FROM nw_taxi.trips_with_zones\n    WHERE pickup_zone IS NOT NULL\n    GROUP BY pickup_zone, pickup_borough\n    ORDER BY trip_count DESC\n    LIMIT 20\n\"\"\")\n\nprint(\"\\nüèÜ Top 20 Pickup Zones:\")\ntop_pickup_zones.show(truncate=False)\n\n# Borough-to-borough flow analysis\nborough_flows = spark.sql(\"\"\"\n    SELECT \n        pickup_borough,\n        dropoff_borough,\n        COUNT(*) as trip_count,\n        AVG(fare_amount) as avg_fare,\n        AVG(trip_distance) as avg_distance,\n        AVG(trip_duration_minutes) as avg_duration,\n        SUM(total_amount) as total_revenue\n    FROM nw_taxi.trips_with_zones\n    WHERE pickup_borough IS NOT NULL AND dropoff_borough IS NOT NULL\n    GROUP BY pickup_borough, dropoff_borough\n    ORDER BY trip_count DESC\n    LIMIT 15\n\"\"\")\n\nprint(\"\\nüîÑ Top Borough-to-Borough Flows:\")\nborough_flows.show(truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Airport analysis\nairport_analysis = spark.sql(\"\"\"\n    SELECT \n        CASE \n            WHEN pickup_zone LIKE '%Airport%' OR pickup_zone LIKE '%JFK%' \n                 OR pickup_zone LIKE '%LaGuardia%' OR pickup_zone LIKE '%LGA%'\n                 OR pickup_zone LIKE '%Newark%' THEN 'From Airport'\n            WHEN dropoff_zone LIKE '%Airport%' OR dropoff_zone LIKE '%JFK%' \n                 OR dropoff_zone LIKE '%LaGuardia%' OR dropoff_zone LIKE '%LGA%'\n                 OR dropoff_zone LIKE '%Newark%' THEN 'To Airport'\n            ELSE 'Non-Airport'\n        END as trip_category,\n        trip_type,\n        COUNT(*) as trip_count,\n        AVG(fare_amount) as avg_fare,\n        AVG(total_amount) as avg_total,\n        AVG(trip_distance) as avg_distance,\n        AVG(trip_duration_minutes) as avg_duration,\n        AVG(tip_percentage) as avg_tip_pct\n    FROM nw_taxi.trips_with_zones\n    GROUP BY trip_category, trip_type\n    ORDER BY trip_count DESC\n\"\"\")\n\nprint(\"\\n‚úàÔ∏è Airport vs Non-Airport Trips:\")\nairport_analysis.show(truncate=False)\n\n# Geographic visualization data\nborough_viz_data = borough_stats.toPandas()\nzone_viz_data = top_pickup_zones.limit(10).toPandas()\n\n# Create geographic visualizations\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n# Borough trip distribution\nborough_trips = borough_viz_data.groupby('pickup_borough')['trip_count'].sum().sort_values(ascending=False)\nborough_trips.plot(kind='bar', ax=axes[0, 0], color='steelblue')\naxes[0, 0].set_xlabel('Borough')\naxes[0, 0].set_ylabel('Number of Trips')\naxes[0, 0].set_title('Trip Volume by Pickup Borough')\naxes[0, 0].tick_params(axis='x', rotation=45)\n\n# Borough revenue distribution\nborough_revenue = borough_viz_data.groupby('pickup_borough')['total_revenue'].sum().sort_values(ascending=False)\nborough_revenue.plot(kind='bar', ax=axes[0, 1], color='green')\naxes[0, 1].set_xlabel('Borough')\naxes[0, 1].set_ylabel('Total Revenue ($)')\naxes[0, 1].set_title('Revenue by Pickup Borough')\naxes[0, 1].tick_params(axis='x', rotation=45)\n\n# Top pickup zones\nzone_labels = [f\"{zone[:20]}...\" if len(zone) > 20 else zone for zone in zone_viz_data['pickup_zone']]\naxes[1, 0].barh(range(len(zone_viz_data)), zone_viz_data['trip_count'], color='orange')\naxes[1, 0].set_yticks(range(len(zone_viz_data)))\naxes[1, 0].set_yticklabels(zone_labels, fontsize=8)\naxes[1, 0].set_xlabel('Number of Trips')\naxes[1, 0].set_title('Top 10 Pickup Zones')\n\n# Trip type distribution by borough\nborough_trip_type = borough_viz_data.pivot(index='pickup_borough', columns='trip_type', values='trip_count').fillna(0)\nborough_trip_type.plot(kind='bar', ax=axes[1, 1], color=['gold', 'green'], stacked=True)\naxes[1, 1].set_xlabel('Borough')\naxes[1, 1].set_ylabel('Number of Trips')\naxes[1, 1].set_title('Trip Type Distribution by Borough')\naxes[1, 1].tick_params(axis='x', rotation=45)\naxes[1, 1].legend(title='Taxi Type')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"‚úÖ Geographic analysis visualizations complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Payment and Pricing Analysis at Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Payment and pricing analysis using Spark SQL\nprint(\"üí≥ Payment and Pricing Analysis\")\nprint(\"=\" * 40)\n\n# Payment method analysis\npayment_stats = spark.sql(\"\"\"\n    SELECT \n        payment_method,\n        trip_type,\n        COUNT(*) as trip_count,\n        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY trip_type), 2) as pct_of_type,\n        AVG(fare_amount) as avg_fare,\n        AVG(tip_amount) as avg_tip,\n        AVG(tip_percentage) as avg_tip_pct,\n        PERCENTILE_APPROX(tip_percentage, 0.5) as median_tip_pct,\n        SUM(total_amount) as total_revenue\n    FROM nw_taxi.trips\n    GROUP BY payment_method, trip_type\n    ORDER BY trip_type, trip_count DESC\n\"\"\")\n\nprint(\"üí≥ Payment Method Analysis:\")\npayment_stats.show(truncate=False)\n\n# Fare analysis by distance bands\ndistance_analysis = spark.sql(\"\"\"\n    SELECT \n        CASE \n            WHEN trip_distance <= 1 THEN '0-1 miles'\n            WHEN trip_distance <= 2 THEN '1-2 miles'\n            WHEN trip_distance <= 5 THEN '2-5 miles'\n            WHEN trip_distance <= 10 THEN '5-10 miles'\n            WHEN trip_distance <= 20 THEN '10-20 miles'\n            ELSE '20+ miles'\n        END as distance_band,\n        trip_type,\n        COUNT(*) as trip_count,\n        AVG(trip_distance) as avg_distance,\n        AVG(fare_amount) as avg_fare,\n        AVG(fare_per_mile) as avg_fare_per_mile,\n        AVG(total_amount) as avg_total,\n        AVG(trip_duration_minutes) as avg_duration,\n        AVG(trip_speed_mph) as avg_speed,\n        PERCENTILE_APPROX(fare_per_mile, 0.5) as median_fare_per_mile\n    FROM nw_taxi.trips\n    GROUP BY distance_band, trip_type\n    ORDER BY trip_type, \n             CASE distance_band\n                 WHEN '0-1 miles' THEN 1\n                 WHEN '1-2 miles' THEN 2\n                 WHEN '2-5 miles' THEN 3\n                 WHEN '5-10 miles' THEN 4\n                 WHEN '10-20 miles' THEN 5\n                 ELSE 6\n             END\n\"\"\")\n\nprint(\"\\nüìè Fare Analysis by Distance Bands:\")\ndistance_analysis.show(truncate=False)\n\n# Tip analysis by time period\ntip_by_time = spark.sql(\"\"\"\n    SELECT \n        time_period,\n        trip_type,\n        COUNT(*) as trip_count,\n        AVG(tip_amount) as avg_tip_amount,\n        AVG(tip_percentage) as avg_tip_pct,\n        PERCENTILE_APPROX(tip_percentage, 0.5) as median_tip_pct,\n        PERCENTILE_APPROX(tip_percentage, 0.95) as p95_tip_pct\n    FROM nw_taxi.trips\n    WHERE payment_method = 'Credit Card'  -- Focus on credit card tips\n    GROUP BY time_period, trip_type\n    ORDER BY avg_tip_pct DESC\n\"\"\")\n\nprint(\"\\nüí∞ Tip Analysis by Time Period (Credit Card Only):\")\ntip_by_time.show(truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create payment and pricing visualizations\n",
    "print(\"üìä Creating payment and pricing visualizations...\")\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "payment_pandas = payment_stats.toPandas()\n",
    "distance_pandas = distance_analysis.toPandas()\n",
    "tip_pandas = tip_by_time.toPandas()\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Payment method distribution (overall)\n",
    "payment_dist = payment_pandas.groupby('payment_method')['trip_count'].sum().sort_values(ascending=False)\n",
    "axes[0, 0].pie(payment_dist.values, labels=payment_dist.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 0].set_title('Payment Method Distribution')\n",
    "\n",
    "# Average tip by payment method\n",
    "tip_by_payment = payment_pandas.groupby('payment_method')['avg_tip_pct'].mean().sort_values(ascending=False)\n",
    "tip_by_payment.plot(kind='bar', ax=axes[0, 1], color='gold')\n",
    "axes[0, 1].set_xlabel('Payment Method')\n",
    "axes[0, 1].set_ylabel('Average Tip Percentage (%)')\n",
    "axes[0, 1].set_title('Average Tip % by Payment Method')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Fare per mile by distance band\n",
    "distance_viz = distance_pandas[distance_pandas['trip_type'] == 'yellow']\n",
    "axes[0, 2].bar(distance_viz['distance_band'], distance_viz['avg_fare_per_mile'], color='steelblue')\n",
    "axes[0, 2].set_xlabel('Distance Band')\n",
    "axes[0, 2].set_ylabel('Average Fare per Mile ($)')\n",
    "axes[0, 2].set_title('Fare per Mile by Distance (Yellow Taxis)')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Trip speed by distance band\n",
    "for trip_type in distance_pandas['trip_type'].unique():\n",
    "    data = distance_pandas[distance_pandas['trip_type'] == trip_type]\n",
    "    axes[1, 0].plot(data['distance_band'], data['avg_speed'], \n",
    "                   marker='o', linewidth=2, label=f'{trip_type.title()} Taxi')\n",
    "\n",
    "axes[1, 0].set_xlabel('Distance Band')\n",
    "axes[1, 0].set_ylabel('Average Speed (mph)')\n",
    "axes[1, 0].set_title('Average Speed by Distance Band')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tips by time period\n",
    "tip_viz = tip_pandas[tip_pandas['trip_type'] == 'yellow']\n",
    "axes[1, 1].bar(tip_viz['time_period'], tip_viz['avg_tip_pct'], color='green')\n",
    "axes[1, 1].set_xlabel('Time Period')\n",
    "axes[1, 1].set_ylabel('Average Tip Percentage (%)')\n",
    "axes[1, 1].set_title('Tips by Time Period (Yellow Taxis)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Revenue by trip type and payment method\n",
    "revenue_by_payment = payment_pandas.pivot_table(\n",
    "    values='total_revenue', \n",
    "    index='payment_method', \n",
    "    columns='trip_type', \n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "revenue_by_payment.plot(kind='bar', ax=axes[1, 2], color=['gold', 'green'])\n",
    "axes[1, 2].set_xlabel('Payment Method')\n",
    "axes[1, 2].set_ylabel('Total Revenue ($)')\n",
    "axes[1, 2].set_title('Revenue by Payment Method and Trip Type')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "axes[1, 2].legend(title='Taxi Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Payment and pricing analysis visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Analytics with Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced analytics using window functions and complex aggregations\nprint(\"üî¨ Advanced Analytics with Spark SQL\")\nprint(\"=\" * 40)\n\n# Monthly trends with growth calculation\nmonthly_trends = spark.sql(\"\"\"\n    WITH monthly_stats AS (\n        SELECT \n            pickup_year,\n            pickup_month,\n            trip_type,\n            COUNT(*) as total_trips,\n            SUM(total_amount) as total_revenue,\n            AVG(fare_amount) as avg_fare,\n            AVG(trip_distance) as avg_distance\n        FROM nw_taxi.trips\n        GROUP BY pickup_year, pickup_month, trip_type\n    )\n    SELECT \n        pickup_year,\n        pickup_month,\n        trip_type,\n        total_trips,\n        total_revenue,\n        avg_fare,\n        avg_distance,\n        LAG(total_trips) OVER (\n            PARTITION BY trip_type \n            ORDER BY pickup_year, pickup_month\n        ) as prev_month_trips,\n        ROUND(\n            (total_trips - LAG(total_trips) OVER (\n                PARTITION BY trip_type \n                ORDER BY pickup_year, pickup_month\n            )) * 100.0 / LAG(total_trips) OVER (\n                PARTITION BY trip_type \n                ORDER BY pickup_year, pickup_month\n            ), 2\n        ) as month_over_month_growth_pct\n    FROM monthly_stats\n    ORDER BY pickup_year, pickup_month, trip_type\n\"\"\")\n\nprint(\"üìà Monthly Trends with Growth:\")\nmonthly_trends.show(truncate=False)\n\n# Percentile analysis for fare distribution\nfare_percentiles = spark.sql(\"\"\"\n    SELECT \n        trip_type,\n        COUNT(*) as total_trips,\n        ROUND(PERCENTILE_APPROX(fare_amount, 0.1), 2) as p10_fare,\n        ROUND(PERCENTILE_APPROX(fare_amount, 0.25), 2) as p25_fare,\n        ROUND(PERCENTILE_APPROX(fare_amount, 0.5), 2) as median_fare,\n        ROUND(PERCENTILE_APPROX(fare_amount, 0.75), 2) as p75_fare,\n        ROUND(PERCENTILE_APPROX(fare_amount, 0.9), 2) as p90_fare,\n        ROUND(PERCENTILE_APPROX(fare_amount, 0.95), 2) as p95_fare,\n        ROUND(PERCENTILE_APPROX(fare_amount, 0.99), 2) as p99_fare,\n        ROUND(AVG(fare_amount), 2) as mean_fare,\n        ROUND(STDDEV(fare_amount), 2) as stddev_fare\n    FROM nw_taxi.trips\n    GROUP BY trip_type\n    ORDER BY trip_type\n\"\"\")\n\nprint(\"\\nüìä Fare Distribution Percentiles:\")\nfare_percentiles.show(truncate=False)\n\n# Rush hour vs non-rush hour comparison\nrush_hour_analysis = spark.sql(\"\"\"\n    SELECT \n        CASE \n            WHEN pickup_hour IN (7, 8, 9, 17, 18, 19) AND NOT is_weekend THEN 'Rush Hour'\n            ELSE 'Non-Rush Hour'\n        END as period_type,\n        trip_type,\n        COUNT(*) as trip_count,\n        AVG(fare_amount) as avg_fare,\n        AVG(trip_distance) as avg_distance,\n        AVG(trip_duration_minutes) as avg_duration,\n        AVG(trip_speed_mph) as avg_speed,\n        AVG(tip_percentage) as avg_tip_pct,\n        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (PARTITION BY trip_type), 2) as pct_of_total\n    FROM nw_taxi.trips\n    GROUP BY period_type, trip_type\n    ORDER BY trip_type, period_type\n\"\"\")\n\nprint(\"\\nüö¶ Rush Hour vs Non-Rush Hour Analysis:\")\nrush_hour_analysis.show(truncate=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Performance optimization analysis\nprint(\"‚ö° Performance and Efficiency Analysis\")\nprint(\"=\" * 40)\n\n# Speed analysis by borough and hour\nspeed_analysis = spark.sql(\"\"\"\n    SELECT \n        pickup_hour,\n        pickup_borough,\n        trip_type,\n        COUNT(*) as trip_count,\n        ROUND(AVG(trip_speed_mph), 2) as avg_speed,\n        ROUND(PERCENTILE_APPROX(trip_speed_mph, 0.5), 2) as median_speed,\n        ROUND(PERCENTILE_APPROX(trip_speed_mph, 0.25), 2) as q1_speed,\n        ROUND(PERCENTILE_APPROX(trip_speed_mph, 0.75), 2) as q3_speed,\n        ROUND(AVG(trip_duration_minutes), 2) as avg_duration\n    FROM nw_taxi.trips_with_zones\n    WHERE pickup_borough IS NOT NULL \n      AND trip_speed_mph BETWEEN 1 AND 60\n    GROUP BY pickup_hour, pickup_borough, trip_type\n    HAVING COUNT(*) >= 100\n    ORDER BY pickup_borough, pickup_hour, trip_type\n\"\"\")\n\nprint(\"üèÉ Speed Analysis by Borough and Hour:\")\nspeed_analysis.filter(col(\"pickup_borough\") == \"Manhattan\").show(20, truncate=False)\n\n# Efficiency ranking by zone\nzone_efficiency = spark.sql(\"\"\"\n    WITH zone_metrics AS (\n        SELECT \n            pickup_zone,\n            pickup_borough,\n            COUNT(*) as trip_count,\n            AVG(trip_speed_mph) as avg_speed,\n            AVG(fare_per_mile) as avg_fare_per_mile,\n            AVG(tip_percentage) as avg_tip_pct,\n            SUM(total_amount) as total_revenue\n        FROM nw_taxi.trips_with_zones\n        WHERE pickup_zone IS NOT NULL\n          AND trip_speed_mph BETWEEN 1 AND 60\n        GROUP BY pickup_zone, pickup_borough\n        HAVING COUNT(*) >= 500\n    )\n    SELECT \n        pickup_zone,\n        pickup_borough,\n        trip_count,\n        ROUND(avg_speed, 2) as avg_speed_mph,\n        ROUND(avg_fare_per_mile, 2) as avg_fare_per_mile,\n        ROUND(avg_tip_pct, 2) as avg_tip_pct,\n        ROUND(total_revenue, 2) as total_revenue,\n        ROW_NUMBER() OVER (ORDER BY avg_speed DESC) as speed_rank,\n        ROW_NUMBER() OVER (ORDER BY avg_fare_per_mile DESC) as fare_rank,\n        ROW_NUMBER() OVER (ORDER BY total_revenue DESC) as revenue_rank\n    FROM zone_metrics\n    ORDER BY avg_speed DESC\n    LIMIT 20\n\"\"\")\n\nprint(\"\\nüèÜ Top 20 Most Efficient Zones (by Speed):\")\nzone_efficiency.show(truncate=False)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Business Intelligence Summary Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate comprehensive business intelligence summary\nprint(\"üìà Business Intelligence Dashboard\")\nprint(\"=\" * 60)\n\n# Executive summary metrics\nexec_summary = spark.sql(\"\"\"\n    SELECT \n        'Overall Performance' as metric_category,\n        COUNT(*) as total_trips,\n        ROUND(SUM(total_amount), 2) as total_revenue,\n        ROUND(AVG(fare_amount), 2) as avg_fare,\n        ROUND(AVG(trip_distance), 2) as avg_distance,\n        ROUND(AVG(tip_percentage), 2) as avg_tip_pct,\n        COUNT(DISTINCT pickup_location_id) as unique_pickup_zones,\n        COUNT(DISTINCT dropoff_location_id) as unique_dropoff_zones,\n        ROUND(AVG(trip_speed_mph), 2) as avg_speed_mph\n    FROM nw_taxi.trips\n\"\"\")\n\nprint(\"üìä EXECUTIVE SUMMARY:\")\nexec_summary.show(truncate=False)\n\n# Market share analysis\nmarket_share = spark.sql(\"\"\"\n    SELECT \n        trip_type,\n        COUNT(*) as trip_count,\n        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as trip_market_share_pct,\n        ROUND(SUM(total_amount), 2) as total_revenue,\n        ROUND(SUM(total_amount) * 100.0 / SUM(SUM(total_amount)) OVER (), 2) as revenue_market_share_pct,\n        ROUND(AVG(fare_amount), 2) as avg_fare,\n        ROUND(AVG(trip_distance), 2) as avg_distance,\n        ROUND(AVG(tip_percentage), 2) as avg_tip_pct\n    FROM nw_taxi.trips\n    GROUP BY trip_type\n    ORDER BY trip_count DESC\n\"\"\")\n\nprint(\"\\nüöñ MARKET SHARE ANALYSIS:\")\nmarket_share.show(truncate=False)\n\n# Peak performance indicators\npeak_indicators = spark.sql(\"\"\"\n    WITH hourly_performance AS (\n        SELECT \n            pickup_hour,\n            COUNT(*) as trips,\n            SUM(total_amount) as revenue\n        FROM nw_taxi.trips\n        GROUP BY pickup_hour\n    ),\n    daily_performance AS (\n        SELECT \n            day_name,\n            COUNT(*) as trips,\n            SUM(total_amount) as revenue\n        FROM nw_taxi.trips\n        GROUP BY day_name\n    )\n    SELECT \n        'Peak Performance' as category,\n        (SELECT pickup_hour FROM hourly_performance ORDER BY trips DESC LIMIT 1) as peak_hour,\n        (SELECT MAX(trips) FROM hourly_performance) as peak_hour_trips,\n        (SELECT day_name FROM daily_performance ORDER BY trips DESC LIMIT 1) as peak_day,\n        (SELECT MAX(trips) FROM daily_performance) as peak_day_trips\n\"\"\")\n\nprint(\"\\nüïê PEAK PERFORMANCE INDICATORS:\")\npeak_indicators.show(truncate=False)\n\n# Revenue insights\nrevenue_insights = spark.sql(\"\"\"\n    WITH payment_revenue AS (\n        SELECT \n            payment_method,\n            SUM(total_amount) as revenue,\n            AVG(tip_percentage) as avg_tip_pct\n        FROM nw_taxi.trips\n        GROUP BY payment_method\n    )\n    SELECT \n        payment_method,\n        ROUND(revenue, 2) as total_revenue,\n        ROUND(revenue * 100.0 / SUM(revenue) OVER (), 2) as revenue_share_pct,\n        ROUND(avg_tip_pct, 2) as avg_tip_pct\n    FROM payment_revenue\n    ORDER BY revenue DESC\n\"\"\")\n\nprint(\"\\nüí∞ REVENUE BY PAYMENT METHOD:\")\nrevenue_insights.show(truncate=False)\n\n# Performance benchmarks\nbenchmarks = spark.sql(\"\"\"\n    SELECT \n        'Performance Benchmarks' as category,\n        ROUND(PERCENTILE_APPROX(trip_speed_mph, 0.5), 2) as median_speed_mph,\n        ROUND(PERCENTILE_APPROX(fare_per_mile, 0.5), 2) as median_fare_per_mile,\n        ROUND(PERCENTILE_APPROX(tip_percentage, 0.5), 2) as median_tip_pct,\n        ROUND(PERCENTILE_APPROX(trip_duration_minutes, 0.5), 2) as median_duration_min,\n        COUNT(DISTINCT CASE WHEN pickup_hour BETWEEN 7 AND 9 OR pickup_hour BETWEEN 17 AND 19 THEN pickup_date END) as rush_hour_days,\n        COUNT(DISTINCT pickup_date) as total_days\n    FROM nw_taxi.trips\n\"\"\")\n\nprint(\"\\n‚ö° PERFORMANCE BENCHMARKS:\")\nbenchmarks.show(truncate=False)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úÖ NYC Taxi Big Data Analysis Complete!\")\nprint(\"üåê Spark UI: Check the provided URL for detailed job execution metrics\")\nprint(\"üìä All analytics performed using distributed Spark processing\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup and Session Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save processed data for future analysis\n",
    "print(\"üíæ Saving processed data for future use...\")\n",
    "\n",
    "# Save enriched dataset (partitioned by trip_type for optimal access)\n",
    "try:\n",
    "    output_path = \"../data/processed/nyc_taxi_enriched_spark\"\n",
    "    valid_trips.write.mode(\"overwrite\").partitionBy(\"trip_type\").parquet(output_path)\n",
    "    print(f\"‚úÖ Enriched data saved to: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not save data: {e}\")\n",
    "\n",
    "# Unpersist cached DataFrames to free memory\n",
    "valid_trips.unpersist()\n",
    "trips_df.unpersist()\n",
    "zones_df.unpersist()\n",
    "\n",
    "print(\"üßπ Cache cleared\")\n",
    "\n",
    "# Print final statistics\n",
    "print(\"\\nüìä Final Analysis Statistics:\")\n",
    "print(f\"   - Analyzed {exec_summary.collect()[0]['total_trips']:,} taxi trips\")\n",
    "print(f\"   - Total revenue processed: ${exec_summary.collect()[0]['total_revenue']:,.2f}\")\n",
    "print(f\"   - Used distributed processing across multiple cores\")\n",
    "print(f\"   - Leveraged Spark SQL for complex analytics\")\n",
    "\n",
    "print(\"\\nüéØ Key Insights Discovered:\")\n",
    "peak_data = peak_indicators.collect()[0]\n",
    "market_data = market_share.collect()\n",
    "\n",
    "print(f\"   - Peak demand hour: {peak_data['peak_hour']}:00 ({peak_data['peak_hour_trips']:,} trips)\")\n",
    "print(f\"   - Peak demand day: {peak_data['peak_day']} ({peak_data['peak_day_trips']:,} trips)\")\n",
    "\n",
    "for row in market_data:\n",
    "    print(f\"   - {row['trip_type'].title()} taxis: {row['trip_market_share_pct']:.1f}% market share\")\n",
    "\n",
    "# Keep Spark session running for interactive use\n",
    "print(\"\\nüöÄ Spark session is still active for additional analysis\")\n",
    "print(\"üí° Use spark.stop() when you're completely done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Advantages of PySpark for NYC Taxi Analysis\n",
    "\n",
    "### üöÄ **Scalability Benefits**\n",
    "- **Distributed Processing**: Handle datasets too large for single-machine memory\n",
    "- **Horizontal Scaling**: Add more cores/machines as data grows\n",
    "- **Lazy Evaluation**: Optimizes query execution plans automatically\n",
    "\n",
    "### ‚ö° **Performance Optimizations**\n",
    "- **In-Memory Caching**: Keep frequently accessed data in RAM\n",
    "- **Columnar Storage**: Efficient parquet reading/writing\n",
    "- **Catalyst Optimizer**: Automatic SQL query optimization\n",
    "- **Code Generation**: JIT compilation for faster execution\n",
    "\n",
    "### üîß **Advanced Analytics Capabilities**\n",
    "- **Window Functions**: Complex time-series and ranking operations\n",
    "- **Built-in ML**: MLlib for machine learning at scale\n",
    "- **Streaming**: Real-time processing capabilities\n",
    "- **Graph Processing**: Network analysis with GraphX\n",
    "\n",
    "### üìä **Big Data Integration**\n",
    "- **Multiple Formats**: Parquet, JSON, CSV, Delta Lake\n",
    "- **Database Connectivity**: JDBC connections to data warehouses\n",
    "- **Cloud Integration**: Native support for S3, HDFS, Azure, GCS\n",
    "- **Kafka Integration**: Real-time streaming from message queues\n",
    "\n",
    "### üíº **Production Ready**\n",
    "- **Fault Tolerance**: Automatic recovery from node failures\n",
    "- **Resource Management**: Dynamic resource allocation\n",
    "- **Monitoring**: Rich metrics and debugging tools\n",
    "- **Security**: Encryption, authentication, and authorization\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: \n",
    "- Scale to full historical dataset (years of data)\n",
    "- Implement machine learning models for demand prediction\n",
    "- Set up streaming analytics for real-time insights\n",
    "- Deploy on cloud clusters for enterprise-scale analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}